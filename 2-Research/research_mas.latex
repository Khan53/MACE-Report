\section{Muti-Agent Systems} % (fold)
\label{sec:mas}
% What is a multi agent system?
% Simulation uncertainty
% TODO Why are we using it?
% TODO Show some alternative simulation models to multi agent systems
% ASSIGNED TO: Adam, Ricky, Bo

\subsection{Formal Definition of the Multi-Agent System} % (fold)
\label{sub:mas_fd}

	A multi-agent system is defined as a 3-tuple\cite{mas_book,fagin_95}
	\begin{equation}
		W = <E,e_0,\tau>
		\label{eq:mas}
	\end{equation}
	where $E$ is the set of all possible environment states the system can be in, $e_0$ is the initial state the environment is in and $\tau$ is the state transformer function which takes the current state of the environment into a next state. \\

	Note that the state transformer function $\tau$ does not necessarily give a next state based on the current state.
	It is convenient to introduce a series of states that represents the history of state transitions from the initial state $e_0$ to the current state, that is a run of the system. \\

	A run $r$ is represented as follows---
	\begin{equation}
		r: e_0 \xrightarrow{\alpha_0} e_1 \xrightarrow{\alpha_1} \ldots \xrightarrow{\alpha_n} e_{n+1} \xrightarrow{\alpha_{n+1}} \ldots
		\label{eq:run}
	\end{equation}
	where $e_i$ and $\alpha_i$ respectively denote the environment states and the actions performed by the agents. \\

	Let $R$ be the set of all possible runs, the state transformer function $\tau$ can be seen as\footnote{This formulation does not imply agents have a complete picture of the environment. It is most likely in many cases impossible for agents to have a complete understanding.}--- \\
	\begin{equation}
		\tau : R \rightarrow \wp (E).
		\label{eq:state_transformer}
	\end{equation}

	It is also noteworthy that $\tau$ gives an element of $\wp (E)$, i.e. a subset $E'$ of $E$, as its output.
	Each element of $E'$ are associated with a probability to be chosen and an environment state $e' \in E'$ would be chosen as the environment state of the next iteration.
	This allows non-deterministic evolution of the system. \\

	Finally the definition of $R$, which bears a resemblance to the same idea of the set of paths in a graph traversal problem, can be reciprocally given using $\tau$ by induction.
	\begin{align}
		& R			= \cup_{i=1}^\infty R_i, \nonumber \\
		& R_0		= \{ < e_0 > \}, \nonumber \\
		& R_{i+1}	= \{ r_i ++ < e_{i+1} > |
			e_{i+1} \in \tau(r_i) \wedge
			r_i \in R_i
		\}.
		\label{eq:set_of_runs}
	\end{align}

	However it is important to recognise that although a run of a system is similar to a path in a graph in many ways, it is in most cases impractical to traverse the multi-agent system with any graph traversal algorithms.
	For one thing doing this may not give us any insightful information of the system, and for another, the number of all environment states could be non finite. \\

% subsection Formal Definition of the Multi-Agent System (end)


\subsection{Agents} % (fold)
\label{sub:agents}


\subsubsection{Abstract Architectures for Agents} % (fold)

	All agents are put into a big dynamic environment which should be non-deterministic if we want to simulate the real world and therefore it may have a finite set of discrete states $E$.

	\begin{equation}
		E = \{ e_0, e_1, e_2, \cdots\}
		\label{e}
	\end{equation}

	Agents are assumed to have a set of actions Ac that can move currently environment state into some other states.
	In this way, agent is interacting with the environment.
	A run $r$ of an agent in an environment is a sequence of interleaved environment states and agent actions. 
	This sequence may be end with environment state or agent action, represented by $R^E$ and $R^{Ac}$ respectively.
	A state transformer function $\tau$ represents the behaviour of the environment:

	\begin{equation}
		\tau : R^{Ac} \rightarrow s(E)
		\label{tau}
	\end{equation}

	If $\tau$($r$) = NULL, it clearly indicates that there is no possible successor states to r, so it is the terminal of the run (game over). 
	Thus the environment is defined in a 3-tuple state: 

	\begin{equation}
		Env = < E, e_0, \tau>
		\label{env}
	\end{equation}

	Where $E$ is the set of states of environment, $e_0$ is the initial state of environment and $\tau$ is the transformer function. 
	Now let's consider the state of agents.
	For an agent, it just performs an particular action according to the run sequence. 
	Agent is a function which maps runs to actions:

	\begin{equation}
		Ag : R^E \rightarrow Ac
		\label{ag}
	\end{equation}

	Agent makes a decision about what action to perform based on the history of how the environment changes over time that it has witnessed to date.
	These data can be stored as a property of the agent's attribute.

% subsubsection Abstract Architectures for Agents (end)


\subsubsection{Purely Reactive Agent} % (fold)
\label{subs:purely_reactive_agent}

	This kind of agents take actions only depends on the currently state of the environment, nothing to do with their history.
	They base their decision making entirely on the present, with no reference at all to the past.
	\begin{equation}
		Action : E \rightarrow Ac
		\label{eq:action_def}
	\end{equation}

	For example: a thermostat is a purely reactive agent. 
	It's job is to keep the temperature constant inside the room. 
	There are two states of the environment such that one is the room is under a threshold value, we assign this as state $e_1$.
	On the other hand, if the temperature is under normal condition, we assign it to state $e_2$.
	So for this particular agent, if it is in environment $e_1$, it just simply takes on the action that turns the heater on.
	Otherwise, in condition of $e_2$, it will turn off the heater.
	In this system, the agent is purely reactive, which is only making decisions based on the currently environment state and nothing to do with the past. 

% subsubsection Purely Reactive Agent (end)


\subsubsection{Perceptive Agent} % (fold)
\label{subs:percept_agent}

	Perceptive agent is slightly more complex than the purely reactive agent.
	It now takes two steps to make decisions. 
	The agent now has the ability to observe the environment with a see function, whereas the action function represents the agent's decision making process.
	% Graph needed here
	The see function maps the environment state E to a percept of the agent's view of the world. 
	\begin{equation}
		See : E \rightarrow Per
		\label{see}
	\end{equation}
	And action now maps sequence of percept to actions instead of mapping environment to actions directly.
	\begin{equation}
		Action : Per^* \rightarrow Ac
		\label{action}
	\end{equation}
	$Per^*$ is a sequence of percept because the agent can record all it have seen in the past which indicates that the agent can learn from the history and then make decisions. 
	Improve this a bit more we get the agent with states.

% subsubsection Perceptive Agent (end)


\subsubsection{Agents with State} % (fold)
\label{subs:agent_with_state}

	% Graph needed here
	These agents can now maintain states that are internal data structures of agents and can be used to record information about the environment state and the history. 
	So we introduce I to be the set of all internal states.
	For this type of agent, the function see doesn't change at all, still maps the environment state E to the percept Per.
	\begin{equation}
		See : E \rightarrow Per
		\label{eq:see}
	\end{equation}

	But for the action part, it is now defined in a different way as the interference of those internal states of agents.
	The action of a agent is actually decided by the set of internal states $I$:
	\begin{equation}
		Action : I \rightarrow Ac
		\label{eq:action}
	\end{equation}

	And how we can use the percept to make some progress on the internal states? Here we introduce a function $next$ which works like a feed back in the digital electronic field.
	This function maps an internal state and a percept to a new internal state.
	It seems like the agent is learning something slowly form the changing environment states and each time varies its internal states as well.
	\begin{equation}
		Next : I \times Per \rightarrow I
		\label{next}
	\end{equation}

	So in conclusion, the agent first observes the environment, then it modifies the internal states according to the concept and the current internal states.
	Depends on this new achieving internal state, the agent can carry on with different actions.
	This action will somehow affect the environment and update the environment states. 
	This goes on as a loop forever until the agent dies.

% subsubsection Agents with State (end)


% subsection Agents


\subsection{Validation of the Agent-Based Simulation} % (fold)
\label{sub:validation}

\subsubsection{Necessity of Validation} % (fold)
\label{subs:validation_necessity}

	% detailed reasons besides structural problems, especially time \& space quantisation
	Recall the definition of a run of a multi-agent system above in \eqref{eq:run}, it is immediately clear that the system evolves in discrete time.
	While this may not appear as an immediate problem with simulation, as finer the time interval between iterations generally implies smaller errors.
	However as agents start making decisions based on a history of the environment states, the quantisation errors at previous iterations begin to accumulate.
	Consider the following scenario: the environment provides a function $f(t)$, which is dependent on time, and an agent in the environment calculates the following value at iteration $n$---
	\begin{equation}
		y_{n} = y_{n-1} + f(t_{n-1})\Delta t_{n-1},
		\quad \text{with~} y_{0} = 0.
	\end{equation}
	Or equivalently,
	\begin{equation}
		y_{n} = \sum_t f(t)\Delta t \approx \int_t f(t)dt.
	\end{equation}
	Hence this is an example of using Euler's method to integrate $f(t)$ over time.
	However it is clear that Euler's method gives imprecise results, which is best illustrated with Figure~\ref{fig:euler}. \\
	\begin{figure}[ht]
		\centering
			\includegraphics[width=0.4\linewidth]{figures/euler}
		\caption{The imprecise nature of Euler's method}
		\small{The smooth line represents the continuous time integration of $f(t)$.
			The line joining dots is the discrete time variation using Euler's method.}
		\label{fig:euler}
	\end{figure}

	% TODO need justification, will be looked at after simulation results.
	In general, the problem with quantisation of time worsens if one must work with agents could give abrupt changes in decisions under certain cases.
	Because this would possibly make outputs in steady state converge to imprecise values, as well as causing phase shifts in oscillatory measurements. 

% subsubsection Necessity of Validation (end)


\subsubsection{Validation With Sensitivity Analysis} % (fold)
\label{subs:validation_sensitivity_analysis}

	One of the methods of validation that can be employed is sensitivity analysis.
	Sensitivity analysis can often give indications of design problems that did not anticipate at the design process.
	It is performed by varying combinations of input parameters of the multi-agent system, and comparing the output results after a number of iterations.
	The sensitivity matrix for the system given input vector $\mathbf{X}_0$ with output vector $\mathbf{Y}$ which is dependent on the input vector, is defined below.
	This matrix can be understood as the change of every output measurements with respect to the changes of every input parameters.
	\begin{align}
		S(\mathbf{X}_0)
		&	=
			\left[
				\nabla_{\mathbf{X}} \mathbf{Y}
			\right]_{\mathbf{X} = \mathbf{X}_0} \nonumber \\
		&	=
			\left[
				\begin{pmatrix}
					\frac{\partial}{\partial X_0} &
					\frac{\partial}{\partial X_1} &
					\ldots &
					\frac{\partial}{\partial X_M}
				\end{pmatrix}^\top \mathbf{Y}^\top
			\right]_{\mathbf{X} = \mathbf{X}_0} \nonumber \\
		&	=
			\begin{bmatrix}
				\frac{\partial Y_0}{\partial X_0} &
				\frac{\partial Y_0}{\partial X_1} &
				\ldots &
				\frac{\partial Y_0}{\partial X_M} \\
				\frac{\partial Y_1}{\partial X_0} &
				\frac{\partial Y_1}{\partial X_1} &
				\ldots &
				\frac{\partial Y_1}{\partial X_M} \\
				\vdots & \vdots & \ddots & \vdots \\
				\frac{\partial Y_N}{\partial X_0} &
				\frac{\partial Y_N}{\partial X_1} &
				\ldots &
				\frac{\partial Y_N}{\partial X_M}
			\end{bmatrix}_{\mathbf{X} = \mathbf{X}_0}
	\end{align}
	where
	\begin{equation*}
		\mathbf{X} =
			\begin{pmatrix}
				X_0 \\ X_1 \\ \vdots \\ X_M
			\end{pmatrix}, \quad
		\mathbf{Y} =
			\begin{pmatrix}
				Y_0 \\ Y_1 \\ \vdots \\ Y_N
			\end{pmatrix},
	\end{equation*}
	both denote the input and output vectors respectively.
	To look for anomalies in the system, one can observe abrupt changes in the euclidean norm of $S$, $||S(\mathbf{X})||$ as one varies the value of $\mathbf{X}$, or abrupt changes in each of the elements of the sensitivity matrix.\footnote{Smaller input/output vectors could make the observation much more manageable.}
	% should this part below belong here?
	Besides validation of the system, the sensitivity matrix can be used as a way to prune insignificant input parameters and output measurements.\cite{mas_validation}
	An input parameter $X_i$ may be pruned if it satisfies the following condition
	\begin{equation}
		\forall \mathbf{X}
		\left(
			\max_{0 \leq j \leq N}
			\frac{\partial Y_j}{\partial X_i}(\mathbf{X})
			< \epsilon
		\right) ,
	\end{equation}
	where $\epsilon$ is some small threshold value, for the reason that all output measurements are changed with insignificant magnitudes in their values, it is thus safe to conclude that the particular input parameter does not impact the system.
	Similarly an output measurement $Y_j$ is of little consequence if it satisfies
	\begin{equation}
		\forall \mathbf{X}
		\left(
			\max_{0 \leq i \leq M}
			\frac{\partial Y_j}{\partial X_i}(\mathbf{X})
			< \epsilon
		\right) ,
	\end{equation}
	since it does not change significant enough for all variances of inputs, it does not provide much insight to the system by changing input parameters.

% subsubsection Sensitivity Analysis (end)


\subsubsection{Comparing Results With Expected Outcomes By Regression Analysis} % (fold)
\label{subs:validation_comparision}

	Besides validation with sensitivity analysis, it is also possible to test the simulation outcomes against some of the well established theories.
	By way of illustration, in a simulated ecosystem one could use the generalised version of the Lotka-Volterra equations\cite{mas_lotka1,mas_lotka2} to perform regression analysis of the populations of all species within the system.
	The generalised Lotka-Volterra equations in a system with $N$ different species are given by
	\begin{equation}
		\frac{dx_i}{dt} =
			\left(
				r_i - \sum_j {a_{ij} x_j}
			\right)
			x_i,
		\label{eq:general_lv}
	\end{equation}
	where $i \in [0,1,\ldots,N-1]$, with $x_i$ of vector $\mathbf{x}$ representing the population of the $i$th specie.
	The value of $r_i$ of the vector $\mathbf{r}$ is the intrinsic growth rate of the $i$th specie.
	It denotes how fast the population of a species grow or decline without interacting with any other species in the system.
	Generally a species with $r \leq 0$ means it must depend on predating other species to survive, while a species with $r \geq 0$ means its population grow could grow indefinitely and must be the target for predation or competition, or its population is unbounded.
	Therefore there exists some $j$th species that either predates it, equivalently, $a_{ij} > 0$ and $a_{ji} < 0$ to control the growth of the $i$th species and the $j$th species must benefit from predation, or competes with it for resources, equivalently, $a_{ij} > 0$ and $a_{ji} > 0$, which means none of them benefits from the growth of each other.\footnote{The could be further discussed when we deal with the ecosystem we defined for the simulation, as stags and hares are assumed to grow intrinsically, and hunters must benefit from hunting them.}
	The matrix $A$ is hence introduced to represent the interaction between species.
	\begin{equation}
		A =
		\begin{pmatrix}
			a_{00} & a_{01} & \ldots \\
			a_{10} & a_{11} & \ldots \\
			\vdots & \vdots & \ddots
		\end{pmatrix}
	\end{equation}

	By finding the change of population of the $i$th species $\Delta x_i$, and the time spent for the current iteration $\Delta t$, it immediately follows that
	\begin{equation}
		\frac{dx_i}{dt} \approx \frac{\Delta x_i}{\Delta t}.
		\label{eq:dxdt_approx}
	\end{equation}

	With equations \eqref{eq:general_lv} and \eqref{eq:dxdt_approx}, one can easily perform linear regression in the $(\mathbf{x},\frac{d\mathbf{x}}{dt})$ space to find the values of the intrinsic growth rate vector $\mathbf{r}$ and the interaction matrix $A$.

% subsubsection Comparing Results With Expected Outcomes (end)


% subsection Validation of the Agent-Based Simu (end)


% section Multi-Agent Systems (end)
